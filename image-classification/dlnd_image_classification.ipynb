{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/cifar/cifar-10-python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc422c07c88>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return x/255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, [None, image_shape[0], image_shape[1], image_shape[2]], 'x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, [None, n_classes], 'y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, None, 'keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_shape = x_tensor.get_shape().as_list()\n",
    "    \n",
    "    # Weights and bias\n",
    "    weight = tf.Variable(tf.truncated_normal((conv_ksize[0],conv_ksize[1],x_shape[3],conv_num_outputs),stddev=0.05))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "\n",
    "    # Apply Convolution\n",
    "    strides=[1,conv_strides[0],conv_strides[1],1]\n",
    "    padding='SAME'\n",
    "    conv_layer = tf.nn.conv2d(x_tensor,weight,strides,padding)\n",
    "    \n",
    "    # Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer,bias)\n",
    "    \n",
    "    # Apply Activation\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    output = tf.nn.max_pool(conv_layer,ksize=[1,pool_ksize[0],pool_ksize[1],1],strides=[1,pool_strides[0],pool_strides[1],1],padding='SAME')\n",
    "    return output\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_shape = x_tensor.get_shape().as_list()\n",
    "    new_shape = x_shape[1]*x_shape[2]*x_shape[3]\n",
    "    return tf.reshape(x_tensor,[-1,new_shape])\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_shape = x_tensor.get_shape().as_list()\n",
    "    \n",
    "    # Weight and bias\n",
    "    weight = tf.Variable(tf.truncated_normal([x_shape[1],num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    # Linear sum\n",
    "    x_tensor = tf.add(tf.matmul(x_tensor,weight),bias)\n",
    "    \n",
    "    # Apply reLU nonlinear activation\n",
    "    return tf.nn.relu(x_tensor)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_shape = x_tensor.get_shape().as_list()\n",
    "    \n",
    "    # Weight and bias\n",
    "    weight = tf.Variable(tf.truncated_normal([x_shape[1],num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    # linear regression output\n",
    "    return tf.add(tf.matmul(x_tensor,weight),bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    x = conv2d_maxpool(x,32,[2,2],[2,2],[2,2],[2,2])\n",
    "\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    x= flatten(x)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    x = fully_conn(x,512)\n",
    "    x = tf.nn.dropout(x,keep_prob)\n",
    "\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    results = output(x,10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return results\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer,feed_dict={\n",
    "        keep_prob:keep_probability,\n",
    "        x: feature_batch,\n",
    "        y: label_batch\n",
    "    })\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = sess.run(cost, feed_dict={\n",
    "                    x: feature_batch,\n",
    "                    y: label_batch,\n",
    "                    keep_prob: 1.})\n",
    "    \n",
    "    valid_acc = sess.run(accuracy, feed_dict={\n",
    "                        x: valid_features,\n",
    "                        y: valid_labels,\n",
    "                        keep_prob: 1.})\n",
    "\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "keep_probability = .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.3185 Validation Accuracy: 0.124000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.2874 Validation Accuracy: 0.123000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.2434 Validation Accuracy: 0.148800\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.2076 Validation Accuracy: 0.166000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.1626 Validation Accuracy: 0.196800\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.1533 Validation Accuracy: 0.186000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     2.1364 Validation Accuracy: 0.238000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     2.1137 Validation Accuracy: 0.239800\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     2.0771 Validation Accuracy: 0.258600\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     2.0559 Validation Accuracy: 0.258600\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     2.0170 Validation Accuracy: 0.277800\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.9503 Validation Accuracy: 0.297200\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.8954 Validation Accuracy: 0.306000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.8734 Validation Accuracy: 0.314800\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.8435 Validation Accuracy: 0.329200\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.8341 Validation Accuracy: 0.334000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.7948 Validation Accuracy: 0.339000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.7323 Validation Accuracy: 0.367200\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.7668 Validation Accuracy: 0.353800\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.7059 Validation Accuracy: 0.374200\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.6957 Validation Accuracy: 0.382400\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.7025 Validation Accuracy: 0.371200\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.6593 Validation Accuracy: 0.391600\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.6038 Validation Accuracy: 0.404200\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.6139 Validation Accuracy: 0.394600\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.5647 Validation Accuracy: 0.406400\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.5350 Validation Accuracy: 0.411600\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.5522 Validation Accuracy: 0.390600\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.5036 Validation Accuracy: 0.404000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.5048 Validation Accuracy: 0.400000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.4636 Validation Accuracy: 0.415200\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.4300 Validation Accuracy: 0.411800\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.4341 Validation Accuracy: 0.411200\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.3841 Validation Accuracy: 0.423600\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.4005 Validation Accuracy: 0.418800\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.3675 Validation Accuracy: 0.426200\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.3606 Validation Accuracy: 0.424600\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.3384 Validation Accuracy: 0.424600\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.3277 Validation Accuracy: 0.424800\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.2962 Validation Accuracy: 0.429400\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.2693 Validation Accuracy: 0.433400\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.2571 Validation Accuracy: 0.438800\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.2336 Validation Accuracy: 0.435200\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.1961 Validation Accuracy: 0.439000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.2189 Validation Accuracy: 0.441000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.2216 Validation Accuracy: 0.434600\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     1.2148 Validation Accuracy: 0.438200\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     1.1520 Validation Accuracy: 0.443200\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     1.1469 Validation Accuracy: 0.443200\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.1286 Validation Accuracy: 0.446800\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.1360 Validation Accuracy: 0.446000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     1.1079 Validation Accuracy: 0.443000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.0641 Validation Accuracy: 0.447000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     1.0729 Validation Accuracy: 0.447600\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     1.0472 Validation Accuracy: 0.450000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     1.0460 Validation Accuracy: 0.453200\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     1.0452 Validation Accuracy: 0.444400\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.9811 Validation Accuracy: 0.455600\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.9723 Validation Accuracy: 0.455000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.9537 Validation Accuracy: 0.450200\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.9260 Validation Accuracy: 0.457800\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.9151 Validation Accuracy: 0.456800\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.8908 Validation Accuracy: 0.454400\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.8824 Validation Accuracy: 0.456600\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.8741 Validation Accuracy: 0.453000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.8420 Validation Accuracy: 0.460200\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.8353 Validation Accuracy: 0.458200\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.8101 Validation Accuracy: 0.459600\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.7929 Validation Accuracy: 0.461600\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.8093 Validation Accuracy: 0.458400\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.7539 Validation Accuracy: 0.465400\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.7982 Validation Accuracy: 0.449400\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.7564 Validation Accuracy: 0.462000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.7347 Validation Accuracy: 0.465600\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.7137 Validation Accuracy: 0.467200\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.7209 Validation Accuracy: 0.464400\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.7292 Validation Accuracy: 0.466200\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.6929 Validation Accuracy: 0.467400\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.6868 Validation Accuracy: 0.464600\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.6870 Validation Accuracy: 0.466200\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.6665 Validation Accuracy: 0.465600\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.6678 Validation Accuracy: 0.468600\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.6223 Validation Accuracy: 0.471400\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.6465 Validation Accuracy: 0.469000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.6348 Validation Accuracy: 0.471000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.6047 Validation Accuracy: 0.471200\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.6032 Validation Accuracy: 0.473000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.5639 Validation Accuracy: 0.473600\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.5711 Validation Accuracy: 0.473600\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.5726 Validation Accuracy: 0.475800\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.5231 Validation Accuracy: 0.475400\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.5265 Validation Accuracy: 0.475000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.4884 Validation Accuracy: 0.472000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.4910 Validation Accuracy: 0.473200\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.4563 Validation Accuracy: 0.478400\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.4541 Validation Accuracy: 0.477200\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.4613 Validation Accuracy: 0.475200\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.4288 Validation Accuracy: 0.477400\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.4418 Validation Accuracy: 0.467800\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.4197 Validation Accuracy: 0.472200\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     0.4065 Validation Accuracy: 0.472400\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     0.3995 Validation Accuracy: 0.466400\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     0.3700 Validation Accuracy: 0.468800\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     0.3489 Validation Accuracy: 0.466200\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     0.3345 Validation Accuracy: 0.471600\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     0.3239 Validation Accuracy: 0.465400\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     0.3163 Validation Accuracy: 0.462800\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     0.3109 Validation Accuracy: 0.464400\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     0.2990 Validation Accuracy: 0.459200\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     0.2809 Validation Accuracy: 0.465800\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     0.2754 Validation Accuracy: 0.464600\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     0.2599 Validation Accuracy: 0.464000\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     0.2463 Validation Accuracy: 0.463600\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     0.2334 Validation Accuracy: 0.463600\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     0.2310 Validation Accuracy: 0.466000\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     0.2217 Validation Accuracy: 0.461400\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     0.2259 Validation Accuracy: 0.463200\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     0.1964 Validation Accuracy: 0.463600\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.1946 Validation Accuracy: 0.463200\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     0.1928 Validation Accuracy: 0.463600\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.1807 Validation Accuracy: 0.467400\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.1744 Validation Accuracy: 0.463800\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.1633 Validation Accuracy: 0.462400\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.1575 Validation Accuracy: 0.469400\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.1513 Validation Accuracy: 0.462400\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.1459 Validation Accuracy: 0.465000\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.1442 Validation Accuracy: 0.461600\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.1366 Validation Accuracy: 0.463200\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     0.1427 Validation Accuracy: 0.471000\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     0.1448 Validation Accuracy: 0.465800\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     0.1325 Validation Accuracy: 0.467800\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     0.1171 Validation Accuracy: 0.465800\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     0.1213 Validation Accuracy: 0.468600\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     0.1222 Validation Accuracy: 0.468200\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     0.1233 Validation Accuracy: 0.467800\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     0.1208 Validation Accuracy: 0.465200\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     0.1066 Validation Accuracy: 0.465800\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     0.1286 Validation Accuracy: 0.465800\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     0.1050 Validation Accuracy: 0.465600\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     0.1173 Validation Accuracy: 0.462600\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     0.0929 Validation Accuracy: 0.463000\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     0.0991 Validation Accuracy: 0.467800\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     0.1087 Validation Accuracy: 0.470400\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     0.1114 Validation Accuracy: 0.456200\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     0.0990 Validation Accuracy: 0.464200\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     0.0986 Validation Accuracy: 0.460600\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     0.0827 Validation Accuracy: 0.462200\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     0.0804 Validation Accuracy: 0.462600\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     0.0761 Validation Accuracy: 0.460400\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     0.0734 Validation Accuracy: 0.461600\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     0.0748 Validation Accuracy: 0.464600\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     0.0818 Validation Accuracy: 0.461400\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     0.0897 Validation Accuracy: 0.469200\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     0.0859 Validation Accuracy: 0.464000\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     0.0768 Validation Accuracy: 0.465200\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     0.0684 Validation Accuracy: 0.463200\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     0.0664 Validation Accuracy: 0.468600\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     0.0666 Validation Accuracy: 0.463000\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     0.0660 Validation Accuracy: 0.470400\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     0.0684 Validation Accuracy: 0.470400\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     0.0662 Validation Accuracy: 0.464600\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     0.0638 Validation Accuracy: 0.467200\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     0.0551 Validation Accuracy: 0.467400\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     0.0585 Validation Accuracy: 0.465800\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     0.0609 Validation Accuracy: 0.465000\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     0.0526 Validation Accuracy: 0.465400\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     0.0515 Validation Accuracy: 0.468000\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     0.0510 Validation Accuracy: 0.467000\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     0.0543 Validation Accuracy: 0.462800\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     0.0483 Validation Accuracy: 0.462400\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     0.0514 Validation Accuracy: 0.465000\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     0.0401 Validation Accuracy: 0.464400\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     0.0438 Validation Accuracy: 0.467400\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     0.0349 Validation Accuracy: 0.467600\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     0.0479 Validation Accuracy: 0.467600\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     0.0408 Validation Accuracy: 0.465400\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     0.0376 Validation Accuracy: 0.467200\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     0.0398 Validation Accuracy: 0.465800\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     0.0446 Validation Accuracy: 0.469200\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     0.0416 Validation Accuracy: 0.465400\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     0.0391 Validation Accuracy: 0.459600\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     0.0400 Validation Accuracy: 0.466800\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     0.0390 Validation Accuracy: 0.461400\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     0.0377 Validation Accuracy: 0.464800\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     0.0419 Validation Accuracy: 0.462200\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     0.0344 Validation Accuracy: 0.465000\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     0.0375 Validation Accuracy: 0.460800\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     0.0303 Validation Accuracy: 0.466600\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     0.0287 Validation Accuracy: 0.465600\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     0.0290 Validation Accuracy: 0.467800\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     0.0331 Validation Accuracy: 0.459200\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     0.0309 Validation Accuracy: 0.467400\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     0.0300 Validation Accuracy: 0.466400\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     0.0286 Validation Accuracy: 0.464200\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     0.0298 Validation Accuracy: 0.465000\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     0.0280 Validation Accuracy: 0.465600\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     0.0292 Validation Accuracy: 0.465200\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     0.0265 Validation Accuracy: 0.467000\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     0.0290 Validation Accuracy: 0.467000\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     0.0245 Validation Accuracy: 0.465800\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2439 Validation Accuracy: 0.115800\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.2874 Validation Accuracy: 0.123200\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.2840 Validation Accuracy: 0.135600\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.2917 Validation Accuracy: 0.171200\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.1050 Validation Accuracy: 0.201200\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.1103 Validation Accuracy: 0.217200\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     2.1422 Validation Accuracy: 0.250000\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.8217 Validation Accuracy: 0.267200\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.9823 Validation Accuracy: 0.272400\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.9496 Validation Accuracy: 0.267600\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.0153 Validation Accuracy: 0.270600\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     2.0080 Validation Accuracy: 0.304600\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.6632 Validation Accuracy: 0.310800\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.8286 Validation Accuracy: 0.313000\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.8855 Validation Accuracy: 0.323400\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.9622 Validation Accuracy: 0.320400\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.9127 Validation Accuracy: 0.334600\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.5924 Validation Accuracy: 0.330000\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.7456 Validation Accuracy: 0.342800\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.8650 Validation Accuracy: 0.346800\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.9383 Validation Accuracy: 0.349600\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.8883 Validation Accuracy: 0.343200\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.5510 Validation Accuracy: 0.358800\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.7009 Validation Accuracy: 0.363200\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.8275 Validation Accuracy: 0.360000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.8883 Validation Accuracy: 0.373000\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.7844 Validation Accuracy: 0.371600\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.5187 Validation Accuracy: 0.358200\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.6479 Validation Accuracy: 0.381400\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.7831 Validation Accuracy: 0.383800\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.8471 Validation Accuracy: 0.393800\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.8078 Validation Accuracy: 0.376800\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.4296 Validation Accuracy: 0.394800\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.5994 Validation Accuracy: 0.388800\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.7376 Validation Accuracy: 0.397400\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.8276 Validation Accuracy: 0.398200\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.7751 Validation Accuracy: 0.395000\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.4061 Validation Accuracy: 0.401600\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.5329 Validation Accuracy: 0.402400\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.7434 Validation Accuracy: 0.406200\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.8085 Validation Accuracy: 0.406400\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.7238 Validation Accuracy: 0.416400\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.3691 Validation Accuracy: 0.407600\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.5640 Validation Accuracy: 0.425800\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.7517 Validation Accuracy: 0.414400\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.7940 Validation Accuracy: 0.412800\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.6639 Validation Accuracy: 0.420200\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.3299 Validation Accuracy: 0.425400\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.4948 Validation Accuracy: 0.422400\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.7204 Validation Accuracy: 0.424000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.7876 Validation Accuracy: 0.428600\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.6180 Validation Accuracy: 0.416600\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.3179 Validation Accuracy: 0.436200\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.4371 Validation Accuracy: 0.437200\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.6865 Validation Accuracy: 0.428200\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.7445 Validation Accuracy: 0.422600\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.5931 Validation Accuracy: 0.429400\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.3083 Validation Accuracy: 0.438600\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.4208 Validation Accuracy: 0.436000\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.6647 Validation Accuracy: 0.441800\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.6967 Validation Accuracy: 0.437800\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.5366 Validation Accuracy: 0.435400\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.2958 Validation Accuracy: 0.428800\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.3617 Validation Accuracy: 0.442400\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.6213 Validation Accuracy: 0.439600\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.6859 Validation Accuracy: 0.440400\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.4736 Validation Accuracy: 0.443800\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.2843 Validation Accuracy: 0.438000\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.3799 Validation Accuracy: 0.435600\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.6336 Validation Accuracy: 0.452200\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.6349 Validation Accuracy: 0.445200\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.4535 Validation Accuracy: 0.447000\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.2382 Validation Accuracy: 0.430600\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.3422 Validation Accuracy: 0.457000\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.5947 Validation Accuracy: 0.454400\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.6736 Validation Accuracy: 0.446400\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.4273 Validation Accuracy: 0.450200\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.1877 Validation Accuracy: 0.455600\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.3226 Validation Accuracy: 0.461400\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.5243 Validation Accuracy: 0.461800\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.6156 Validation Accuracy: 0.456600\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.3396 Validation Accuracy: 0.465000\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     1.1722 Validation Accuracy: 0.455400\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.2623 Validation Accuracy: 0.453000\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.4900 Validation Accuracy: 0.466200\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.6075 Validation Accuracy: 0.470400\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.2810 Validation Accuracy: 0.472600\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     1.1361 Validation Accuracy: 0.459000\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.2930 Validation Accuracy: 0.475600\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.4678 Validation Accuracy: 0.465600\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.6100 Validation Accuracy: 0.465000\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.2894 Validation Accuracy: 0.471000\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     1.0899 Validation Accuracy: 0.473200\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.2243 Validation Accuracy: 0.473600\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.4924 Validation Accuracy: 0.466200\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.5743 Validation Accuracy: 0.479200\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     1.1821 Validation Accuracy: 0.478800\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     1.0546 Validation Accuracy: 0.477200\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.2298 Validation Accuracy: 0.479200\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     1.4034 Validation Accuracy: 0.469800\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.5560 Validation Accuracy: 0.472600\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     1.1758 Validation Accuracy: 0.481800\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.9863 Validation Accuracy: 0.484200\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     1.2383 Validation Accuracy: 0.490200\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     1.4382 Validation Accuracy: 0.469000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.5287 Validation Accuracy: 0.482600\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     1.1016 Validation Accuracy: 0.488600\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.9746 Validation Accuracy: 0.492400\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     1.1710 Validation Accuracy: 0.487400\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     1.3340 Validation Accuracy: 0.481000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.4968 Validation Accuracy: 0.491200\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     1.0983 Validation Accuracy: 0.494000\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.9409 Validation Accuracy: 0.492600\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     1.1753 Validation Accuracy: 0.488600\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     1.3594 Validation Accuracy: 0.476800\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.4613 Validation Accuracy: 0.492000\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     1.0613 Validation Accuracy: 0.497600\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.8916 Validation Accuracy: 0.496200\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     1.1462 Validation Accuracy: 0.490200\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     1.2712 Validation Accuracy: 0.480000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.4351 Validation Accuracy: 0.492600\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     1.0055 Validation Accuracy: 0.490800\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.9135 Validation Accuracy: 0.501000\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     1.1065 Validation Accuracy: 0.497600\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     1.2643 Validation Accuracy: 0.485200\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.4093 Validation Accuracy: 0.485200\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.9568 Validation Accuracy: 0.506600\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.8994 Validation Accuracy: 0.510600\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     1.0960 Validation Accuracy: 0.503000\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     1.2151 Validation Accuracy: 0.484800\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.3911 Validation Accuracy: 0.508000\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.9462 Validation Accuracy: 0.506000\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.8774 Validation Accuracy: 0.507400\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     1.0598 Validation Accuracy: 0.509600\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     1.1936 Validation Accuracy: 0.497800\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.3618 Validation Accuracy: 0.509400\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.9248 Validation Accuracy: 0.512400\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.8729 Validation Accuracy: 0.508200\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     1.0169 Validation Accuracy: 0.516000\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     1.1185 Validation Accuracy: 0.514200\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.3114 Validation Accuracy: 0.515400\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.8980 Validation Accuracy: 0.515000\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.8358 Validation Accuracy: 0.519000\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.9980 Validation Accuracy: 0.521400\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     1.1194 Validation Accuracy: 0.490000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.3012 Validation Accuracy: 0.513200\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.9128 Validation Accuracy: 0.520400\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.7864 Validation Accuracy: 0.520800\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.9518 Validation Accuracy: 0.511400\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     1.0995 Validation Accuracy: 0.506800\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.2607 Validation Accuracy: 0.521400\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.8443 Validation Accuracy: 0.525200\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.7854 Validation Accuracy: 0.524000\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.9335 Validation Accuracy: 0.526200\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     1.0237 Validation Accuracy: 0.522000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.2571 Validation Accuracy: 0.518200\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.8404 Validation Accuracy: 0.533200\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.7353 Validation Accuracy: 0.532200\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.9127 Validation Accuracy: 0.533200\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     1.0423 Validation Accuracy: 0.509200\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.2208 Validation Accuracy: 0.520400\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.7953 Validation Accuracy: 0.534000\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.6980 Validation Accuracy: 0.526800\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.8646 Validation Accuracy: 0.523800\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.9506 Validation Accuracy: 0.526400\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.2052 Validation Accuracy: 0.526600\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.7655 Validation Accuracy: 0.530600\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.7138 Validation Accuracy: 0.532800\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.8351 Validation Accuracy: 0.527200\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.9594 Validation Accuracy: 0.528000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.1636 Validation Accuracy: 0.531600\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.7384 Validation Accuracy: 0.534200\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.6458 Validation Accuracy: 0.535200\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.8217 Validation Accuracy: 0.534000\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.9361 Validation Accuracy: 0.533600\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.1401 Validation Accuracy: 0.532600\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.6859 Validation Accuracy: 0.537600\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.6520 Validation Accuracy: 0.539200\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.8580 Validation Accuracy: 0.530400\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.9041 Validation Accuracy: 0.530600\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.1086 Validation Accuracy: 0.537400\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.7063 Validation Accuracy: 0.532200\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.6196 Validation Accuracy: 0.543600\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.8042 Validation Accuracy: 0.526800\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.9039 Validation Accuracy: 0.532600\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.0531 Validation Accuracy: 0.537000\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.6770 Validation Accuracy: 0.541800\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.6193 Validation Accuracy: 0.543200\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.7504 Validation Accuracy: 0.542600\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.8510 Validation Accuracy: 0.537800\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.0413 Validation Accuracy: 0.536800\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.6560 Validation Accuracy: 0.539800\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.5792 Validation Accuracy: 0.543400\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.7532 Validation Accuracy: 0.543800\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.8204 Validation Accuracy: 0.540400\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.0269 Validation Accuracy: 0.546000\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.6234 Validation Accuracy: 0.548000\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.5685 Validation Accuracy: 0.549400\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.7053 Validation Accuracy: 0.534800\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.8280 Validation Accuracy: 0.544800\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.9995 Validation Accuracy: 0.538000\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.6132 Validation Accuracy: 0.539600\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.5390 Validation Accuracy: 0.554000\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.6731 Validation Accuracy: 0.539600\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.7632 Validation Accuracy: 0.547200\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.9750 Validation Accuracy: 0.539600\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.6163 Validation Accuracy: 0.545000\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.5196 Validation Accuracy: 0.552000\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.6638 Validation Accuracy: 0.547800\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.7818 Validation Accuracy: 0.553000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.9600 Validation Accuracy: 0.546000\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.6016 Validation Accuracy: 0.549800\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.5430 Validation Accuracy: 0.541800\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.6533 Validation Accuracy: 0.545200\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.7849 Validation Accuracy: 0.544600\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.9427 Validation Accuracy: 0.550200\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.5664 Validation Accuracy: 0.547600\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.5014 Validation Accuracy: 0.554800\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.6422 Validation Accuracy: 0.542000\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.7287 Validation Accuracy: 0.544200\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.9082 Validation Accuracy: 0.546400\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.5933 Validation Accuracy: 0.542200\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.5025 Validation Accuracy: 0.553000\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.6161 Validation Accuracy: 0.549400\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.7226 Validation Accuracy: 0.547200\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.8814 Validation Accuracy: 0.550000\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.5390 Validation Accuracy: 0.547200\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.4914 Validation Accuracy: 0.550400\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.5943 Validation Accuracy: 0.546000\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.7115 Validation Accuracy: 0.551400\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.8576 Validation Accuracy: 0.554400\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.5148 Validation Accuracy: 0.556400\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.4895 Validation Accuracy: 0.556000\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.5630 Validation Accuracy: 0.556800\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.7268 Validation Accuracy: 0.549800\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.8235 Validation Accuracy: 0.559000\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.4992 Validation Accuracy: 0.552400\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.4459 Validation Accuracy: 0.557400\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.5927 Validation Accuracy: 0.548200\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.6703 Validation Accuracy: 0.554400\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.7983 Validation Accuracy: 0.558000\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.4828 Validation Accuracy: 0.547800\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.4823 Validation Accuracy: 0.551400\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.5511 Validation Accuracy: 0.549000\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.6887 Validation Accuracy: 0.552400\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.8236 Validation Accuracy: 0.557800\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.4812 Validation Accuracy: 0.551400\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.4590 Validation Accuracy: 0.547200\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.5523 Validation Accuracy: 0.556800\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.7009 Validation Accuracy: 0.535800\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.7511 Validation Accuracy: 0.562600\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.4284 Validation Accuracy: 0.550800\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.4208 Validation Accuracy: 0.557400\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.5191 Validation Accuracy: 0.553600\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.6375 Validation Accuracy: 0.551400\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.7841 Validation Accuracy: 0.556600\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.4201 Validation Accuracy: 0.550200\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.3802 Validation Accuracy: 0.563800\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.5043 Validation Accuracy: 0.561200\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.5926 Validation Accuracy: 0.563400\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.7525 Validation Accuracy: 0.559000\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.4140 Validation Accuracy: 0.556800\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.3960 Validation Accuracy: 0.557400\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.5152 Validation Accuracy: 0.552800\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.5730 Validation Accuracy: 0.556800\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.7714 Validation Accuracy: 0.564200\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.4094 Validation Accuracy: 0.554400\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.3750 Validation Accuracy: 0.567200\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.4786 Validation Accuracy: 0.554600\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.5857 Validation Accuracy: 0.556400\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.7551 Validation Accuracy: 0.559200\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.3944 Validation Accuracy: 0.555600\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.3911 Validation Accuracy: 0.559800\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.4711 Validation Accuracy: 0.564800\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.5502 Validation Accuracy: 0.562200\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.7133 Validation Accuracy: 0.566600\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.4192 Validation Accuracy: 0.557600\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.3606 Validation Accuracy: 0.564000\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.4581 Validation Accuracy: 0.566200\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.5399 Validation Accuracy: 0.556400\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.7330 Validation Accuracy: 0.551400\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.4042 Validation Accuracy: 0.552200\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.3451 Validation Accuracy: 0.563400\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.4886 Validation Accuracy: 0.554000\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.5103 Validation Accuracy: 0.566600\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.6734 Validation Accuracy: 0.560000\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.4143 Validation Accuracy: 0.560200\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.3400 Validation Accuracy: 0.566200\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.4142 Validation Accuracy: 0.557000\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.4754 Validation Accuracy: 0.559200\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.6538 Validation Accuracy: 0.560800\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.3655 Validation Accuracy: 0.557800\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.3109 Validation Accuracy: 0.569800\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.4606 Validation Accuracy: 0.555600\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.4804 Validation Accuracy: 0.559600\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.6631 Validation Accuracy: 0.558000\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.3448 Validation Accuracy: 0.551200\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.3099 Validation Accuracy: 0.566600\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.4539 Validation Accuracy: 0.551600\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.4721 Validation Accuracy: 0.567600\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.7204 Validation Accuracy: 0.547600\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.3288 Validation Accuracy: 0.558600\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.3135 Validation Accuracy: 0.568600\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.3898 Validation Accuracy: 0.563600\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.4648 Validation Accuracy: 0.565000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.6501 Validation Accuracy: 0.557400\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.3233 Validation Accuracy: 0.551800\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.3254 Validation Accuracy: 0.564600\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.4192 Validation Accuracy: 0.561200\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.4663 Validation Accuracy: 0.561000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.6097 Validation Accuracy: 0.556800\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.2951 Validation Accuracy: 0.558600\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.3048 Validation Accuracy: 0.569200\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.4422 Validation Accuracy: 0.561600\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.4556 Validation Accuracy: 0.565600\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.6525 Validation Accuracy: 0.551600\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.3223 Validation Accuracy: 0.551600\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.2862 Validation Accuracy: 0.564400\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.4205 Validation Accuracy: 0.554000\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.4460 Validation Accuracy: 0.564800\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.5539 Validation Accuracy: 0.558600\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.2952 Validation Accuracy: 0.558400\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.2879 Validation Accuracy: 0.562000\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.3968 Validation Accuracy: 0.560200\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.4650 Validation Accuracy: 0.557800\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.5972 Validation Accuracy: 0.555400\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.2729 Validation Accuracy: 0.556800\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.2831 Validation Accuracy: 0.566600\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.4155 Validation Accuracy: 0.558800\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.4126 Validation Accuracy: 0.569800\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.5985 Validation Accuracy: 0.550800\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.2583 Validation Accuracy: 0.562000\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.2561 Validation Accuracy: 0.567200\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.4162 Validation Accuracy: 0.555800\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.3690 Validation Accuracy: 0.568800\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.5200 Validation Accuracy: 0.556400\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.2535 Validation Accuracy: 0.558200\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.2613 Validation Accuracy: 0.564000\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.4000 Validation Accuracy: 0.562200\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.3822 Validation Accuracy: 0.568400\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.5547 Validation Accuracy: 0.554800\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.2594 Validation Accuracy: 0.560600\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.2531 Validation Accuracy: 0.567600\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.3953 Validation Accuracy: 0.558400\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.3848 Validation Accuracy: 0.568000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.5167 Validation Accuracy: 0.553400\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.2458 Validation Accuracy: 0.560400\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.2540 Validation Accuracy: 0.568400\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.3982 Validation Accuracy: 0.555800\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.3562 Validation Accuracy: 0.574400\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.5162 Validation Accuracy: 0.557400\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.2715 Validation Accuracy: 0.562400\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.2607 Validation Accuracy: 0.571400\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.3904 Validation Accuracy: 0.550600\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.3552 Validation Accuracy: 0.570200\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.4783 Validation Accuracy: 0.561800\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.2179 Validation Accuracy: 0.557400\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.2548 Validation Accuracy: 0.572000\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.3795 Validation Accuracy: 0.557000\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.3539 Validation Accuracy: 0.571600\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.4739 Validation Accuracy: 0.562400\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.2229 Validation Accuracy: 0.560600\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.2243 Validation Accuracy: 0.570200\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.3580 Validation Accuracy: 0.561200\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.3406 Validation Accuracy: 0.576600\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.4773 Validation Accuracy: 0.561000\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.2261 Validation Accuracy: 0.560200\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.2550 Validation Accuracy: 0.564600\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.3583 Validation Accuracy: 0.556600\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.3289 Validation Accuracy: 0.577000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.4492 Validation Accuracy: 0.562400\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.2109 Validation Accuracy: 0.560000\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.2345 Validation Accuracy: 0.569800\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.3884 Validation Accuracy: 0.559000\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.3301 Validation Accuracy: 0.576800\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.4285 Validation Accuracy: 0.559400\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.2058 Validation Accuracy: 0.563800\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.2412 Validation Accuracy: 0.573800\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.3829 Validation Accuracy: 0.551400\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.3471 Validation Accuracy: 0.567000\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.4228 Validation Accuracy: 0.560800\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.1900 Validation Accuracy: 0.564600\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.2217 Validation Accuracy: 0.568800\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.3329 Validation Accuracy: 0.560600\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.3169 Validation Accuracy: 0.573400\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.4117 Validation Accuracy: 0.556200\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.1829 Validation Accuracy: 0.561000\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.2218 Validation Accuracy: 0.568800\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.3633 Validation Accuracy: 0.556000\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.3127 Validation Accuracy: 0.573200\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.3979 Validation Accuracy: 0.552400\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.1712 Validation Accuracy: 0.569000\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.2131 Validation Accuracy: 0.566200\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.3312 Validation Accuracy: 0.557800\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.2978 Validation Accuracy: 0.572400\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.3591 Validation Accuracy: 0.563400\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.1878 Validation Accuracy: 0.569200\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.2103 Validation Accuracy: 0.568400\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.3009 Validation Accuracy: 0.568800\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.3097 Validation Accuracy: 0.573000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.3547 Validation Accuracy: 0.562600\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.1882 Validation Accuracy: 0.569800\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.2185 Validation Accuracy: 0.563200\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.3105 Validation Accuracy: 0.572400\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.2852 Validation Accuracy: 0.577800\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.3464 Validation Accuracy: 0.559200\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.1832 Validation Accuracy: 0.565200\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.2089 Validation Accuracy: 0.571000\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.2909 Validation Accuracy: 0.567400\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.2875 Validation Accuracy: 0.572000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.3590 Validation Accuracy: 0.562400\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.1744 Validation Accuracy: 0.566400\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.2135 Validation Accuracy: 0.562400\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.3060 Validation Accuracy: 0.566000\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.2914 Validation Accuracy: 0.573800\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.3521 Validation Accuracy: 0.556000\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.1655 Validation Accuracy: 0.564400\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.1953 Validation Accuracy: 0.568000\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.2849 Validation Accuracy: 0.563000\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.2721 Validation Accuracy: 0.574600\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.3268 Validation Accuracy: 0.564800\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.1674 Validation Accuracy: 0.569600\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.1955 Validation Accuracy: 0.571600\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.2779 Validation Accuracy: 0.568200\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.2879 Validation Accuracy: 0.567600\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.3024 Validation Accuracy: 0.560600\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.1622 Validation Accuracy: 0.564000\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.1889 Validation Accuracy: 0.571200\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.2575 Validation Accuracy: 0.568400\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.2646 Validation Accuracy: 0.576600\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.3138 Validation Accuracy: 0.564000\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.1659 Validation Accuracy: 0.565600\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.1913 Validation Accuracy: 0.569200\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.2736 Validation Accuracy: 0.566200\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.2535 Validation Accuracy: 0.572200\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.2800 Validation Accuracy: 0.558200\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.1528 Validation Accuracy: 0.574000\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.1853 Validation Accuracy: 0.570200\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.2596 Validation Accuracy: 0.577400\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.2592 Validation Accuracy: 0.573600\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.2962 Validation Accuracy: 0.556800\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.1546 Validation Accuracy: 0.568400\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.1941 Validation Accuracy: 0.566200\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.2701 Validation Accuracy: 0.567200\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.2408 Validation Accuracy: 0.568800\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.2848 Validation Accuracy: 0.563200\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.1333 Validation Accuracy: 0.570600\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.1834 Validation Accuracy: 0.574000\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.2420 Validation Accuracy: 0.573400\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.2696 Validation Accuracy: 0.569000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.2799 Validation Accuracy: 0.555200\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.1355 Validation Accuracy: 0.567800\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.1960 Validation Accuracy: 0.558000\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.2670 Validation Accuracy: 0.570000\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.2650 Validation Accuracy: 0.572400\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.2474 Validation Accuracy: 0.560200\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.1226 Validation Accuracy: 0.573400\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.1875 Validation Accuracy: 0.566200\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.2395 Validation Accuracy: 0.574400\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.2647 Validation Accuracy: 0.576000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.2697 Validation Accuracy: 0.562200\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.1333 Validation Accuracy: 0.571400\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.1719 Validation Accuracy: 0.566200\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.2509 Validation Accuracy: 0.574200\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.2387 Validation Accuracy: 0.574400\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.3208 Validation Accuracy: 0.554400\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.1307 Validation Accuracy: 0.570800\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.1954 Validation Accuracy: 0.567600\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.2259 Validation Accuracy: 0.571800\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.2509 Validation Accuracy: 0.573200\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.2793 Validation Accuracy: 0.562600\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.1329 Validation Accuracy: 0.572200\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.1802 Validation Accuracy: 0.571800\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.2208 Validation Accuracy: 0.569400\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.2446 Validation Accuracy: 0.574200\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.2551 Validation Accuracy: 0.567000\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.1334 Validation Accuracy: 0.571200\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.1923 Validation Accuracy: 0.572200\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.2170 Validation Accuracy: 0.569800\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.2731 Validation Accuracy: 0.574600\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.2406 Validation Accuracy: 0.560600\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.1289 Validation Accuracy: 0.562600\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.1805 Validation Accuracy: 0.570200\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.2182 Validation Accuracy: 0.573000\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.2863 Validation Accuracy: 0.570000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.2380 Validation Accuracy: 0.562800\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.1191 Validation Accuracy: 0.568200\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.1599 Validation Accuracy: 0.576200\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.2131 Validation Accuracy: 0.573000\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.2394 Validation Accuracy: 0.577600\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.2416 Validation Accuracy: 0.564600\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.1177 Validation Accuracy: 0.558800\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.1618 Validation Accuracy: 0.563600\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.2003 Validation Accuracy: 0.569600\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.2512 Validation Accuracy: 0.569200\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.2490 Validation Accuracy: 0.561400\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.1191 Validation Accuracy: 0.562800\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.1553 Validation Accuracy: 0.568200\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.2022 Validation Accuracy: 0.567000\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.2392 Validation Accuracy: 0.572600\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     0.2381 Validation Accuracy: 0.559200\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:     0.1172 Validation Accuracy: 0.566000\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:     0.1526 Validation Accuracy: 0.567200\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:     0.1933 Validation Accuracy: 0.571600\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:     0.2126 Validation Accuracy: 0.570400\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     0.2221 Validation Accuracy: 0.559400\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:     0.1381 Validation Accuracy: 0.552200\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:     0.1565 Validation Accuracy: 0.569000\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:     0.1777 Validation Accuracy: 0.566600\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:     0.2069 Validation Accuracy: 0.571400\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     0.2132 Validation Accuracy: 0.561800\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:     0.1018 Validation Accuracy: 0.557200\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:     0.1637 Validation Accuracy: 0.569400\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:     0.1865 Validation Accuracy: 0.568800\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:     0.2116 Validation Accuracy: 0.571400\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     0.2144 Validation Accuracy: 0.564400\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:     0.1186 Validation Accuracy: 0.563000\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:     0.1468 Validation Accuracy: 0.567400\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss:     0.1794 Validation Accuracy: 0.571400\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:     0.1973 Validation Accuracy: 0.569000\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     0.2311 Validation Accuracy: 0.561600\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:     0.0963 Validation Accuracy: 0.561600\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:     0.1527 Validation Accuracy: 0.566800\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:     0.1813 Validation Accuracy: 0.566000\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:     0.1955 Validation Accuracy: 0.569200\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     0.2276 Validation Accuracy: 0.561400\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:     0.1033 Validation Accuracy: 0.559000\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:     0.1434 Validation Accuracy: 0.566200\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss:     0.1689 Validation Accuracy: 0.565800\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:     0.1780 Validation Accuracy: 0.569200\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     0.2435 Validation Accuracy: 0.560200\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:     0.0927 Validation Accuracy: 0.562400\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:     0.1476 Validation Accuracy: 0.567000\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:     0.1629 Validation Accuracy: 0.566800\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss:     0.1830 Validation Accuracy: 0.574000\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     0.2035 Validation Accuracy: 0.562600\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:     0.0932 Validation Accuracy: 0.564400\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:     0.1482 Validation Accuracy: 0.564000\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:     0.1692 Validation Accuracy: 0.565800\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss:     0.1783 Validation Accuracy: 0.567000\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     0.2023 Validation Accuracy: 0.565200\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:     0.0955 Validation Accuracy: 0.562000\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:     0.1380 Validation Accuracy: 0.564400\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:     0.1620 Validation Accuracy: 0.567200\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:     0.1725 Validation Accuracy: 0.572000\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     0.2104 Validation Accuracy: 0.559200\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:     0.0825 Validation Accuracy: 0.562400\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:     0.1398 Validation Accuracy: 0.563400\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:     0.1483 Validation Accuracy: 0.567800\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:     0.1617 Validation Accuracy: 0.569000\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     0.1810 Validation Accuracy: 0.565000\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:     0.0874 Validation Accuracy: 0.559800\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:     0.1455 Validation Accuracy: 0.568400\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:     0.1479 Validation Accuracy: 0.566600\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:     0.1595 Validation Accuracy: 0.570800\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     0.2126 Validation Accuracy: 0.562000\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:     0.0894 Validation Accuracy: 0.560200\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:     0.1357 Validation Accuracy: 0.570200\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:     0.1507 Validation Accuracy: 0.565200\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:     0.1490 Validation Accuracy: 0.566000\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     0.2017 Validation Accuracy: 0.566400\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:     0.0832 Validation Accuracy: 0.559800\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:     0.1411 Validation Accuracy: 0.565600\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:     0.1579 Validation Accuracy: 0.564000\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:     0.1477 Validation Accuracy: 0.562800\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     0.2068 Validation Accuracy: 0.566000\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:     0.0770 Validation Accuracy: 0.564000\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:     0.1337 Validation Accuracy: 0.564800\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:     0.1496 Validation Accuracy: 0.563400\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:     0.1596 Validation Accuracy: 0.566400\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     0.1917 Validation Accuracy: 0.564000\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:     0.0758 Validation Accuracy: 0.561400\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:     0.1426 Validation Accuracy: 0.564000\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:     0.1378 Validation Accuracy: 0.567800\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:     0.1557 Validation Accuracy: 0.562600\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     0.2055 Validation Accuracy: 0.562400\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:     0.0734 Validation Accuracy: 0.558800\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:     0.1242 Validation Accuracy: 0.571400\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:     0.1445 Validation Accuracy: 0.562000\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:     0.1414 Validation Accuracy: 0.561400\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     0.1960 Validation Accuracy: 0.561800\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:     0.0648 Validation Accuracy: 0.565800\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:     0.1175 Validation Accuracy: 0.563800\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:     0.1324 Validation Accuracy: 0.564600\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:     0.1468 Validation Accuracy: 0.561400\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     0.1849 Validation Accuracy: 0.568000\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:     0.0701 Validation Accuracy: 0.561400\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:     0.1274 Validation Accuracy: 0.567600\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:     0.1195 Validation Accuracy: 0.563800\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:     0.1399 Validation Accuracy: 0.558600\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.2068 Validation Accuracy: 0.562000\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:     0.0652 Validation Accuracy: 0.560200\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:     0.1185 Validation Accuracy: 0.567200\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:     0.1292 Validation Accuracy: 0.564200\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:     0.1381 Validation Accuracy: 0.560600\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     0.1752 Validation Accuracy: 0.566800\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:     0.0630 Validation Accuracy: 0.564800\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:     0.1112 Validation Accuracy: 0.571200\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:     0.1220 Validation Accuracy: 0.564200\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:     0.1253 Validation Accuracy: 0.561400\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.1663 Validation Accuracy: 0.564200\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:     0.0537 Validation Accuracy: 0.559600\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:     0.1136 Validation Accuracy: 0.571200\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:     0.1148 Validation Accuracy: 0.565400\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:     0.1235 Validation Accuracy: 0.564000\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.1957 Validation Accuracy: 0.566400\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:     0.0620 Validation Accuracy: 0.557800\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:     0.1133 Validation Accuracy: 0.568200\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:     0.1197 Validation Accuracy: 0.567400\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:     0.1205 Validation Accuracy: 0.561200\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.1520 Validation Accuracy: 0.560800\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:     0.0529 Validation Accuracy: 0.556200\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:     0.1161 Validation Accuracy: 0.568800\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:     0.1092 Validation Accuracy: 0.563600\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:     0.1203 Validation Accuracy: 0.557400\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.1557 Validation Accuracy: 0.560800\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:     0.0532 Validation Accuracy: 0.565400\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:     0.1097 Validation Accuracy: 0.570600\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:     0.1094 Validation Accuracy: 0.567400\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:     0.1266 Validation Accuracy: 0.561800\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.1868 Validation Accuracy: 0.557600\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss:     0.0518 Validation Accuracy: 0.559600\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss:     0.1118 Validation Accuracy: 0.571200\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss:     0.1148 Validation Accuracy: 0.565200\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss:     0.1194 Validation Accuracy: 0.559000\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.1755 Validation Accuracy: 0.563400\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss:     0.0632 Validation Accuracy: 0.556400\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss:     0.1184 Validation Accuracy: 0.558200\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss:     0.1115 Validation Accuracy: 0.564000\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss:     0.1168 Validation Accuracy: 0.560200\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.1658 Validation Accuracy: 0.555600\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss:     0.0470 Validation Accuracy: 0.564000\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss:     0.1058 Validation Accuracy: 0.568000\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss:     0.1147 Validation Accuracy: 0.563600\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss:     0.1059 Validation Accuracy: 0.556800\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.1555 Validation Accuracy: 0.557800\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss:     0.0470 Validation Accuracy: 0.557600\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss:     0.1094 Validation Accuracy: 0.567600\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss:     0.1057 Validation Accuracy: 0.564800\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss:     0.1109 Validation Accuracy: 0.559000\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     0.1687 Validation Accuracy: 0.564000\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss:     0.0526 Validation Accuracy: 0.560800\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss:     0.1069 Validation Accuracy: 0.570600\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss:     0.1043 Validation Accuracy: 0.562400\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss:     0.1125 Validation Accuracy: 0.556000\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     0.1701 Validation Accuracy: 0.562600\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss:     0.0414 Validation Accuracy: 0.567800\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss:     0.1135 Validation Accuracy: 0.564000\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss:     0.1062 Validation Accuracy: 0.558200\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss:     0.1069 Validation Accuracy: 0.551000\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     0.1321 Validation Accuracy: 0.562800\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss:     0.0462 Validation Accuracy: 0.556800\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss:     0.1061 Validation Accuracy: 0.563600\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss:     0.0971 Validation Accuracy: 0.561200\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss:     0.1052 Validation Accuracy: 0.551600\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     0.1560 Validation Accuracy: 0.558400\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss:     0.0452 Validation Accuracy: 0.559000\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss:     0.1064 Validation Accuracy: 0.564200\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss:     0.0978 Validation Accuracy: 0.562600\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss:     0.0971 Validation Accuracy: 0.559600\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     0.1392 Validation Accuracy: 0.562200\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss:     0.0442 Validation Accuracy: 0.558000\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss:     0.0981 Validation Accuracy: 0.561600\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss:     0.0884 Validation Accuracy: 0.556400\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss:     0.0884 Validation Accuracy: 0.558200\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     0.1388 Validation Accuracy: 0.562400\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss:     0.0472 Validation Accuracy: 0.558000\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss:     0.1067 Validation Accuracy: 0.557400\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss:     0.0939 Validation Accuracy: 0.562000\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss:     0.0915 Validation Accuracy: 0.557400\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     0.1491 Validation Accuracy: 0.562400\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss:     0.0430 Validation Accuracy: 0.555800\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss:     0.1016 Validation Accuracy: 0.560400\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss:     0.0862 Validation Accuracy: 0.564400\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss:     0.0883 Validation Accuracy: 0.552800\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     0.1485 Validation Accuracy: 0.564400\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss:     0.0442 Validation Accuracy: 0.557400\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss:     0.1022 Validation Accuracy: 0.563200\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss:     0.0857 Validation Accuracy: 0.558000\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss:     0.0845 Validation Accuracy: 0.558200\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     0.1430 Validation Accuracy: 0.560600\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss:     0.0435 Validation Accuracy: 0.552000\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss:     0.1025 Validation Accuracy: 0.556000\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss:     0.0964 Validation Accuracy: 0.557200\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss:     0.0828 Validation Accuracy: 0.556200\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     0.1565 Validation Accuracy: 0.557000\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss:     0.0397 Validation Accuracy: 0.558800\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss:     0.1030 Validation Accuracy: 0.560400\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss:     0.0919 Validation Accuracy: 0.553000\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss:     0.0883 Validation Accuracy: 0.551200\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     0.1239 Validation Accuracy: 0.562800\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss:     0.0382 Validation Accuracy: 0.556800\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss:     0.0962 Validation Accuracy: 0.561400\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss:     0.0903 Validation Accuracy: 0.563800\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss:     0.0812 Validation Accuracy: 0.558600\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     0.1432 Validation Accuracy: 0.556600\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss:     0.0374 Validation Accuracy: 0.560000\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss:     0.0985 Validation Accuracy: 0.558400\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss:     0.0856 Validation Accuracy: 0.552000\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss:     0.0754 Validation Accuracy: 0.558200\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     0.1201 Validation Accuracy: 0.558200\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss:     0.0452 Validation Accuracy: 0.557200\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss:     0.1021 Validation Accuracy: 0.556600\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss:     0.0906 Validation Accuracy: 0.558400\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss:     0.0769 Validation Accuracy: 0.557800\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     0.1386 Validation Accuracy: 0.558000\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss:     0.0339 Validation Accuracy: 0.559600\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss:     0.0998 Validation Accuracy: 0.559800\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss:     0.0949 Validation Accuracy: 0.556400\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss:     0.0769 Validation Accuracy: 0.556000\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     0.1653 Validation Accuracy: 0.551400\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss:     0.0344 Validation Accuracy: 0.554200\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss:     0.0937 Validation Accuracy: 0.553400\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss:     0.0864 Validation Accuracy: 0.557200\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss:     0.0885 Validation Accuracy: 0.551800\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     0.1272 Validation Accuracy: 0.561000\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss:     0.0356 Validation Accuracy: 0.559600\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss:     0.1006 Validation Accuracy: 0.543800\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss:     0.0887 Validation Accuracy: 0.554800\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss:     0.0797 Validation Accuracy: 0.553000\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     0.1352 Validation Accuracy: 0.554000\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss:     0.0425 Validation Accuracy: 0.558000\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss:     0.0841 Validation Accuracy: 0.558200\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss:     0.0817 Validation Accuracy: 0.559800\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss:     0.0750 Validation Accuracy: 0.550600\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     0.1744 Validation Accuracy: 0.546800\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss:     0.0359 Validation Accuracy: 0.556600\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss:     0.0837 Validation Accuracy: 0.550200\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss:     0.0939 Validation Accuracy: 0.560000\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss:     0.0705 Validation Accuracy: 0.551800\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     0.1465 Validation Accuracy: 0.547000\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss:     0.0353 Validation Accuracy: 0.557800\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss:     0.0827 Validation Accuracy: 0.554400\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss:     0.0919 Validation Accuracy: 0.553200\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss:     0.0787 Validation Accuracy: 0.556200\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     0.1309 Validation Accuracy: 0.553800\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss:     0.0311 Validation Accuracy: 0.555400\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss:     0.0883 Validation Accuracy: 0.550400\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss:     0.0901 Validation Accuracy: 0.556600\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss:     0.0751 Validation Accuracy: 0.555600\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     0.1449 Validation Accuracy: 0.547400\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss:     0.0359 Validation Accuracy: 0.556200\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss:     0.0864 Validation Accuracy: 0.557000\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss:     0.0830 Validation Accuracy: 0.552200\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss:     0.0695 Validation Accuracy: 0.550800\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     0.1465 Validation Accuracy: 0.545200\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss:     0.0388 Validation Accuracy: 0.552800\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss:     0.0876 Validation Accuracy: 0.564400\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss:     0.0826 Validation Accuracy: 0.553600\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss:     0.0686 Validation Accuracy: 0.554400\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     0.1546 Validation Accuracy: 0.539800\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss:     0.0297 Validation Accuracy: 0.557200\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss:     0.0828 Validation Accuracy: 0.559800\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss:     0.0778 Validation Accuracy: 0.556000\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss:     0.0703 Validation Accuracy: 0.549400\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     0.1557 Validation Accuracy: 0.541200\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss:     0.0369 Validation Accuracy: 0.558200\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss:     0.0838 Validation Accuracy: 0.559200\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss:     0.0818 Validation Accuracy: 0.549200\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss:     0.0679 Validation Accuracy: 0.550000\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     0.1172 Validation Accuracy: 0.547400\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss:     0.0337 Validation Accuracy: 0.551600\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss:     0.0861 Validation Accuracy: 0.562600\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss:     0.0731 Validation Accuracy: 0.558400\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss:     0.0613 Validation Accuracy: 0.554400\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     0.1216 Validation Accuracy: 0.545400\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss:     0.0312 Validation Accuracy: 0.555400\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss:     0.0806 Validation Accuracy: 0.557200\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss:     0.0773 Validation Accuracy: 0.553200\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss:     0.0612 Validation Accuracy: 0.548600\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     0.1218 Validation Accuracy: 0.544400\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss:     0.0296 Validation Accuracy: 0.554800\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss:     0.0816 Validation Accuracy: 0.560400\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss:     0.0808 Validation Accuracy: 0.554400\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss:     0.0639 Validation Accuracy: 0.547600\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     0.1298 Validation Accuracy: 0.542000\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss:     0.0308 Validation Accuracy: 0.554200\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss:     0.0843 Validation Accuracy: 0.560600\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss:     0.0801 Validation Accuracy: 0.560600\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss:     0.0649 Validation Accuracy: 0.553200\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     0.0969 Validation Accuracy: 0.546000\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss:     0.0312 Validation Accuracy: 0.554000\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss:     0.0773 Validation Accuracy: 0.560800\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss:     0.0829 Validation Accuracy: 0.553400\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss:     0.0619 Validation Accuracy: 0.553600\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     0.1168 Validation Accuracy: 0.547600\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss:     0.0252 Validation Accuracy: 0.559200\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss:     0.0764 Validation Accuracy: 0.561400\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss:     0.0743 Validation Accuracy: 0.556400\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss:     0.0602 Validation Accuracy: 0.549000\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     0.1170 Validation Accuracy: 0.541600\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss:     0.0258 Validation Accuracy: 0.548800\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss:     0.0860 Validation Accuracy: 0.560400\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss:     0.0800 Validation Accuracy: 0.559000\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss:     0.0600 Validation Accuracy: 0.549400\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     0.1298 Validation Accuracy: 0.540000\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss:     0.0307 Validation Accuracy: 0.544800\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss:     0.0762 Validation Accuracy: 0.562200\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss:     0.0679 Validation Accuracy: 0.554400\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss:     0.0626 Validation Accuracy: 0.548400\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     0.1347 Validation Accuracy: 0.536400\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss:     0.0283 Validation Accuracy: 0.555600\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss:     0.0923 Validation Accuracy: 0.565600\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss:     0.0758 Validation Accuracy: 0.549800\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss:     0.0578 Validation Accuracy: 0.549200\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     0.1205 Validation Accuracy: 0.540200\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss:     0.0299 Validation Accuracy: 0.545400\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss:     0.0827 Validation Accuracy: 0.556600\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss:     0.0804 Validation Accuracy: 0.556400\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss:     0.0589 Validation Accuracy: 0.546400\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     0.1359 Validation Accuracy: 0.538800\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss:     0.0320 Validation Accuracy: 0.546800\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss:     0.0885 Validation Accuracy: 0.565800\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss:     0.0806 Validation Accuracy: 0.556400\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss:     0.0544 Validation Accuracy: 0.546400\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     0.1077 Validation Accuracy: 0.545600\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss:     0.0275 Validation Accuracy: 0.550200\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss:     0.0867 Validation Accuracy: 0.561000\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss:     0.0763 Validation Accuracy: 0.553400\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss:     0.0531 Validation Accuracy: 0.551600\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     0.1128 Validation Accuracy: 0.537400\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss:     0.0243 Validation Accuracy: 0.550600\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss:     0.0861 Validation Accuracy: 0.556400\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss:     0.0752 Validation Accuracy: 0.552800\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss:     0.0622 Validation Accuracy: 0.545800\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     0.1190 Validation Accuracy: 0.542000\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss:     0.0250 Validation Accuracy: 0.545800\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss:     0.0866 Validation Accuracy: 0.561800\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss:     0.0845 Validation Accuracy: 0.548000\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss:     0.0629 Validation Accuracy: 0.546800\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     0.1031 Validation Accuracy: 0.539800\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss:     0.0212 Validation Accuracy: 0.545800\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss:     0.0883 Validation Accuracy: 0.552400\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss:     0.0783 Validation Accuracy: 0.545600\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss:     0.0590 Validation Accuracy: 0.545200\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     0.0963 Validation Accuracy: 0.539400\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss:     0.0221 Validation Accuracy: 0.549000\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss:     0.0781 Validation Accuracy: 0.562000\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss:     0.0772 Validation Accuracy: 0.551800\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss:     0.0545 Validation Accuracy: 0.548000\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     0.1033 Validation Accuracy: 0.537400\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss:     0.0243 Validation Accuracy: 0.539000\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss:     0.0829 Validation Accuracy: 0.558200\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss:     0.0707 Validation Accuracy: 0.554200\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss:     0.0614 Validation Accuracy: 0.545600\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     0.0934 Validation Accuracy: 0.542200\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss:     0.0201 Validation Accuracy: 0.543800\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss:     0.0857 Validation Accuracy: 0.558400\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss:     0.0655 Validation Accuracy: 0.548600\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss:     0.0708 Validation Accuracy: 0.547800\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     0.1060 Validation Accuracy: 0.534000\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss:     0.0207 Validation Accuracy: 0.540400\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss:     0.0900 Validation Accuracy: 0.557600\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss:     0.0707 Validation Accuracy: 0.544000\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss:     0.0594 Validation Accuracy: 0.551800\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     0.0832 Validation Accuracy: 0.539200\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss:     0.0204 Validation Accuracy: 0.544400\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss:     0.0712 Validation Accuracy: 0.560200\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss:     0.0617 Validation Accuracy: 0.553200\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss:     0.0529 Validation Accuracy: 0.546400\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     0.0825 Validation Accuracy: 0.541000\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss:     0.0188 Validation Accuracy: 0.548600\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss:     0.0872 Validation Accuracy: 0.563800\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss:     0.0700 Validation Accuracy: 0.547800\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss:     0.0630 Validation Accuracy: 0.555400\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     0.0884 Validation Accuracy: 0.537800\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss:     0.0205 Validation Accuracy: 0.540600\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss:     0.0893 Validation Accuracy: 0.555800\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss:     0.0719 Validation Accuracy: 0.549400\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss:     0.0687 Validation Accuracy: 0.553400\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     0.0954 Validation Accuracy: 0.532600\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss:     0.0190 Validation Accuracy: 0.545600\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss:     0.0839 Validation Accuracy: 0.557600\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss:     0.0713 Validation Accuracy: 0.550000\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss:     0.0509 Validation Accuracy: 0.552600\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     0.0953 Validation Accuracy: 0.532600\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss:     0.0167 Validation Accuracy: 0.548600\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss:     0.0789 Validation Accuracy: 0.557200\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss:     0.0640 Validation Accuracy: 0.544600\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss:     0.0492 Validation Accuracy: 0.550600\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     0.0754 Validation Accuracy: 0.545600\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss:     0.0172 Validation Accuracy: 0.551400\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss:     0.0831 Validation Accuracy: 0.556600\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss:     0.0633 Validation Accuracy: 0.549400\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss:     0.0529 Validation Accuracy: 0.555000\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     0.0727 Validation Accuracy: 0.542400\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss:     0.0179 Validation Accuracy: 0.551400\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss:     0.0797 Validation Accuracy: 0.559800\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss:     0.0699 Validation Accuracy: 0.551600\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss:     0.0576 Validation Accuracy: 0.549200\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     0.0745 Validation Accuracy: 0.535800\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss:     0.0187 Validation Accuracy: 0.546600\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss:     0.0816 Validation Accuracy: 0.557200\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss:     0.0640 Validation Accuracy: 0.553400\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss:     0.0538 Validation Accuracy: 0.551800\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     0.0613 Validation Accuracy: 0.535800\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss:     0.0176 Validation Accuracy: 0.546400\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss:     0.0887 Validation Accuracy: 0.562000\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss:     0.0621 Validation Accuracy: 0.551800\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss:     0.0539 Validation Accuracy: 0.551800\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     0.0698 Validation Accuracy: 0.547000\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss:     0.0164 Validation Accuracy: 0.551200\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss:     0.0671 Validation Accuracy: 0.554200\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss:     0.0627 Validation Accuracy: 0.549800\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss:     0.0537 Validation Accuracy: 0.552200\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     0.0703 Validation Accuracy: 0.541000\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss:     0.0180 Validation Accuracy: 0.547800\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss:     0.0716 Validation Accuracy: 0.561400\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss:     0.0695 Validation Accuracy: 0.545400\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss:     0.0547 Validation Accuracy: 0.549600\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     0.0546 Validation Accuracy: 0.547200\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss:     0.0182 Validation Accuracy: 0.554200\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss:     0.0687 Validation Accuracy: 0.556400\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss:     0.0638 Validation Accuracy: 0.547000\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss:     0.0518 Validation Accuracy: 0.550000\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     0.0643 Validation Accuracy: 0.543800\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss:     0.0143 Validation Accuracy: 0.548200\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss:     0.0845 Validation Accuracy: 0.559200\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss:     0.0540 Validation Accuracy: 0.546200\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss:     0.0498 Validation Accuracy: 0.554400\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     0.0544 Validation Accuracy: 0.547200\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss:     0.0167 Validation Accuracy: 0.547400\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss:     0.0724 Validation Accuracy: 0.554400\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss:     0.0791 Validation Accuracy: 0.544400\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss:     0.0435 Validation Accuracy: 0.547200\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     0.0645 Validation Accuracy: 0.547600\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss:     0.0148 Validation Accuracy: 0.549400\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss:     0.0694 Validation Accuracy: 0.557600\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss:     0.0595 Validation Accuracy: 0.546400\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss:     0.0445 Validation Accuracy: 0.552400\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     0.0574 Validation Accuracy: 0.541800\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss:     0.0141 Validation Accuracy: 0.549600\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss:     0.0795 Validation Accuracy: 0.553600\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss:     0.0601 Validation Accuracy: 0.550400\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss:     0.0484 Validation Accuracy: 0.550000\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     0.0500 Validation Accuracy: 0.542800\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss:     0.0150 Validation Accuracy: 0.551600\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss:     0.0767 Validation Accuracy: 0.558600\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss:     0.0549 Validation Accuracy: 0.550200\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss:     0.0437 Validation Accuracy: 0.558000\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     0.0591 Validation Accuracy: 0.542200\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss:     0.0160 Validation Accuracy: 0.550400\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss:     0.0740 Validation Accuracy: 0.560000\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss:     0.0602 Validation Accuracy: 0.548000\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss:     0.0458 Validation Accuracy: 0.547200\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     0.0567 Validation Accuracy: 0.546200\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss:     0.0147 Validation Accuracy: 0.554800\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss:     0.0746 Validation Accuracy: 0.557200\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss:     0.0651 Validation Accuracy: 0.549000\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss:     0.0484 Validation Accuracy: 0.552000\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     0.0607 Validation Accuracy: 0.542400\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss:     0.0139 Validation Accuracy: 0.552400\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss:     0.0738 Validation Accuracy: 0.561600\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss:     0.0596 Validation Accuracy: 0.547000\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss:     0.0443 Validation Accuracy: 0.554400\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     0.0549 Validation Accuracy: 0.545200\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss:     0.0178 Validation Accuracy: 0.549800\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss:     0.0629 Validation Accuracy: 0.560200\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss:     0.0516 Validation Accuracy: 0.542400\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss:     0.0437 Validation Accuracy: 0.550800\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     0.0567 Validation Accuracy: 0.544800\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss:     0.0176 Validation Accuracy: 0.549000\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss:     0.0681 Validation Accuracy: 0.558600\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss:     0.0475 Validation Accuracy: 0.545400\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss:     0.0419 Validation Accuracy: 0.556800\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     0.0443 Validation Accuracy: 0.550800\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss:     0.0202 Validation Accuracy: 0.550800\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss:     0.0630 Validation Accuracy: 0.561600\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss:     0.0507 Validation Accuracy: 0.543800\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss:     0.0512 Validation Accuracy: 0.552400\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     0.0433 Validation Accuracy: 0.551600\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss:     0.0150 Validation Accuracy: 0.546800\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss:     0.0620 Validation Accuracy: 0.558600\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss:     0.0533 Validation Accuracy: 0.542600\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss:     0.0473 Validation Accuracy: 0.552600\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     0.0436 Validation Accuracy: 0.548200\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss:     0.0129 Validation Accuracy: 0.549200\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss:     0.0628 Validation Accuracy: 0.562200\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss:     0.0465 Validation Accuracy: 0.552200\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss:     0.0395 Validation Accuracy: 0.551200\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     0.0443 Validation Accuracy: 0.547000\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss:     0.0146 Validation Accuracy: 0.553000\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss:     0.0649 Validation Accuracy: 0.560200\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss:     0.0471 Validation Accuracy: 0.544800\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss:     0.0484 Validation Accuracy: 0.550400\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     0.0413 Validation Accuracy: 0.548600\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss:     0.0160 Validation Accuracy: 0.552800\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss:     0.0615 Validation Accuracy: 0.557400\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss:     0.0498 Validation Accuracy: 0.550200\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss:     0.0482 Validation Accuracy: 0.550600\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     0.0414 Validation Accuracy: 0.550200\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss:     0.0160 Validation Accuracy: 0.551200\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss:     0.0649 Validation Accuracy: 0.558800\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss:     0.0461 Validation Accuracy: 0.544400\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss:     0.0484 Validation Accuracy: 0.550400\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     0.0394 Validation Accuracy: 0.547800\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss:     0.0160 Validation Accuracy: 0.549800\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss:     0.0610 Validation Accuracy: 0.557000\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss:     0.0486 Validation Accuracy: 0.541800\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss:     0.0487 Validation Accuracy: 0.556800\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.5712025316455697\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc3bdfea828>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
